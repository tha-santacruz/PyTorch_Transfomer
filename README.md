# PyTorch Implementation of a Transformer Network
## Work in progress
This repository contains a personal project in construction.

## Project Description
The goal of the project is to implement a Transformer network from scratch. This is achieved by following the explainations provided in the original paper [1].
The implemented modules are : 

1. The MLP
2. The Attention Head
3. The Multi-Head Attention block
4. The Masked Attention Head
5. The Masked Multi-Head Attention block
6. The Transformer Encoder Layer
7. The Transformer Decoder Layer
8. The Transformer Embedding Layer (positional and tokens)


## References
[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.